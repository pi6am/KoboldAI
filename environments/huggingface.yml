name: koboldai
channels:
  - pytorch
  - nvidia/label/cuda-11.8.0
  - conda-forge
  - defaults
dependencies:
  - colorama
  - flask=2.3.3
  - flask-socketio=5.3.2
  - flask-session=0.5.0
  - python-socketio=5.7.2
  - pytorch=2.1.*
  - python=3.8.*
  - pytorch-cuda=11.8
  - cuda-nvcc=11.8
  - cuda-libraries-dev=11.8
  - eventlet=0.33.3
  - dnspython=2.2.1
  - markdown
  - bleach=4.1.0
  - pip
  - git=2.35.1
  - protobuf
  - marshmallow>=3.13
  - apispec-webframeworks
  - loguru
  - termcolor
  - Pillow
  - psutil
  - ffmpeg
  - pip:
    - flask-cloudflared==0.0.10
    - flask-ngrok
    - flask-cors
    - Werkzeug==2.3.7
    - lupa==1.10
    - transformers[sentencepiece]==4.36.1
    - huggingface_hub==0.19.4
    - optimum[onnxruntime]==1.16.1
    - safetensors==0.4.1
    - accelerate==0.25.0
    - git+https://github.com/VE-FORBRYDERNE/mkultra
    - flask-session
    - ansi2html
    - flask_compress
    - ijson
    - bitsandbytes==0.40.0.post4; sys_platform == 'linux'
    - https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.40.0.post4-py3-none-win_amd64.whl; sys_platform == 'win32'
    - ftfy
    - pydub
    - diffusers
    - git+https://github.com/0cc4m/hf_bleeding_edge/
    - https://github.com/0cc4m/GPTQ-for-LLaMa/releases/download/0.0.6/gptq_koboldai-0.0.6-cp38-cp38-linux_x86_64.whl; sys_platform == 'linux'
    - https://github.com/0cc4m/GPTQ-for-LLaMa/releases/download/0.0.6/gptq_koboldai-0.0.6-cp38-cp38-win_amd64.whl; sys_platform == 'win32'
    - https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.5.1%2Bcu118-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl; sys_platform == 'linux'
    - https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.5.1%2Bcu118-cp38-cp38-win_amd64.whl; sys_platform == 'win32'
    - https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.8/autoawq-0.1.8+cu118-cp38-cp38-linux_x86_64.whl; sys_platform == 'linux'
    - https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.8/autoawq-0.1.8+cu118-cp38-cp38-win_amd64.whl; sys_platform == 'win32'
    - einops
    - peft==0.7.1
    - scipy
    - https://github.com/0cc4m/exllama/releases/download/0.0.8/exllama-0.0.8-cp38-cp38-linux_x86_64.whl; sys_platform == 'linux'
    - https://github.com/0cc4m/exllama/releases/download/0.0.8/exllama-0.0.8-cp38-cp38-win_amd64.whl; sys_platform == 'win32'
    - https://github.com/turboderp/exllamav2/releases/download/v0.0.10/exllamav2-0.0.10+cu118-cp38-cp38-linux_x86_64.whl; sys_platform == 'linux'
    - https://github.com/turboderp/exllamav2/releases/download/v0.0.10/exllamav2-0.0.10+cu118-cp38-cp38-win_amd64.whl; sys_platform == 'win32'
    - windows-curses; sys_platform == 'win32'
    - pynvml
    - xformers==0.0.23.post1
    - https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.6/flash_attn-2.3.6+cu118torch2.1cxx11abiFALSE-cp38-cp38-linux_x86_64.whl; sys_platform == 'linux'
    - omegaconf
